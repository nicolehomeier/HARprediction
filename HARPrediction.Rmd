---
title: "HARPrediction"
author: "Nicole Homeier"
date: "11/1/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# HAR Prediction Exercise for the PML Course
```{r load}
library(caret)
library(randomForest)
options(warn=-1)
```
First read the data in and remove set up variables that can't be used for prediction.
```{r read}
ds = read.csv("pml-training.csv")
test = read.csv("pml-testing.csv")
ds = ds[,-(1:7)]
```
had some trouble with numbers read in as factors due to missing values, so do this
```{r cleanup1}
nds = ds
for (i in 1:(ncol(ds)-1)){ #skip the classe var
    if (class(ds[,i])=="factor"){nds[,i] = as.numeric(as.character(ds[,i]),verbose=FALSE)}
}
```

check for many missing values and remove

```{r remove1}
ctna = sapply(ds,function(x) sum(is.na(x)))
nds = ds[,-which(ctna>5000)]
namesnds = names(ds)[which(ctna>5000)]
```
check out the test set, no use in predicting on variable that aren't available in the test set
```{r checktest}
ctnatest = sapply(test, function(x) sum(is.na(x)))
namestest = names(test)[which(ctnatest==0)]
cnds = nds[,which(names(nds) %in% namestest)]
cnds$classe = nds$classe
```
now have a better set of variables for fitting, but we can do better
check where correlation between predictors is high, remove when needed
```{r corr}
M = cor(cnds[,-ncol(cnds)])
highcorr=which(M>.8 & M <1,arr.ind=T)
names(cnds)[highcorr[,2]]
names(highcorr[,1])
dropvars = c("roll_belt","accel_belt_y","magnet_belt_x","gyros_forearm_z","magnet_arm_x","accel_dumbbell_x","magnet_arm_y","accel_dumbbell_z")
dpts = which(names(cnds) %in% dropvars)
cnds = cnds[,-dpts]
```

do a glm fit to get rid of the remaining non-important "predictors"
```{r glmcull}
glmfit = glm(classe~.,data=cnds,family=binomial)
summary(glmfit)
dropvars = c("gyros_belt_y","gyros_arm_x","gyros_arm_z","pitch_dumbbell","accel_dumbbell_y","magnet_forearm_z")
dpts = which(names(cnds) %in% dropvars)
cnds = cnds[,-dpts]
```
write out in case want to start here later
```{r writeout}
write.csv(cnds,"reduced-pml-training.csv",row.names=F)
```

now create some training and testing data and train some models
```{r trainit}
set.seed(3233)
inTrain = createDataPartition(cnds$classe,p=.6,list=TRUE)[[1]]
training = cnds[inTrain,]
testing = cnds[-inTrain,]
```
```{r treemodel}
rpart.grid = expand.grid(cp=seq(0,0.01,0.0005))
rpartfit = train(classe~.,data=training,method="rpart",tuneGrid=rpart.grid,trControl=trainControl(method="cv"))
predrpart = predict(rpartfit,newdata=testing)
confusionMatrix(predrpart,testing$classe)
```
```{r plrpart}
plot(rpartfit)
```
```{r ldamodel}
ldafit = train(classe~.,data=training,method="lda",preProcess=c("center","scale"),trControl=trainControl(method="cv"))
predlda = predict(ldafit,newdata=testing)
confusionMatrix(predlda,testing$classe)
```

```{r knnmodel}
knn.grid = expand.grid(k=seq(3,12,2))
knnfit = train(classe~.,data=training,method="knn",preProcess=c("center","scale"),tuneGrid=knn.grid,trControl=trainControl(method="cv"))
predknn = predict(knnfit,newdata=testing)
confusionMatrix(predknn,testing$classe)
```
```{r plknn}
plot(knnfit)
```

```{r randomforestmodel}
rffit = randomForest(classe~.,data=training)
predrf = predict(rffit,newdata=testing)
confusionMatrix(predrf,testing$classe)
```
```{r rffit,echo=FALSE}
plot(rffit)
```

can see that don't need more than 200 trees

```{r mtry}
rffit$mtry
```
it would appear that a random forest approach gives the best results, but outside of this exercise it would make me worry about overfitting. Here I've held back 40% of the data so unlikely. Now do some estimation so see how expectations for accuracy on the 20 test set values.

```{r model20}
inTrain = createDataPartition(cnds$classe,p=(1-20/nrow(cnds)),list=TRUE)[[1]]
training = cnds[inTrain,]
testing = cnds[-inTrain,]
```
```{r finalrffit}
finalfit = randomForest(classe~.,data=training,mtry=6,ntree=200,keep.forest=TRUE)
predfinal = predict(finalfit,newdata=testing)
confusionMatrix(predfinal,testing$classe)
predict(finalfit,newdata=test)
```
this final prediction is submitted for the quiz. if this is correct and not subject to overfitting, then the out of sample error should be very low. it could be zero for a sample of 20. a better estimate from a larger sample would be the estimate earlier from training/testing on 60%/40% respectively which showed out of sample error rates of <~0.6%.